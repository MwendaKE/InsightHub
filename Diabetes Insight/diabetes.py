"""
Diabetes Insight Report Generator
Single script to analyze the scikit-learn diabetes dataset and produce a PDF report.

Outputs:
  - Diabetes_Insight_Report_Advanced.pdf  (in the script working directory)

Author: Generated by your assistant (example). Adapt as needed.
"""

import os
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages

from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression

# Optional Bayesian imports (PyMC)
try:
    import pymc as pm
    import arviz as az
    HAS_PYMC = True
except Exception:
    HAS_PYMC = False

# ------------- Configuration -------------
OUT_DIR = "report_output"
PDF_FILENAME = os.path.join(OUT_DIR, "Diabetes_Insight_Report_Advanced.pdf")
os.makedirs(OUT_DIR, exist_ok=True)
RANDOM_STATE = 42
N_TOP_FEATURES = 5  # for simple models and reporting
plt.style.use("seaborn-whitegrid")

# ------------- Helper functions -------------
def load_and_prepare():
    """Load diabetes dataset and return a clean DataFrame."""
    raw = load_diabetes(as_frame=True)
    df = raw.frame.copy()
    # Clean column names for readability and avoid special chars
    df.columns = [c.replace(" ", "_").replace("(", "").replace(")", "").replace("/", "_") for c in df.columns]
    return df, raw

def summarize_data(df):
    """Return quick summaries and correlation info."""
    n_rows, n_cols = df.shape
    missing = df.isna().sum().sum()
    desc = df.describe().T
    corr = df.corr()
    target_corr = corr['target'].sort_values(ascending=False)
    top_pos = list(target_corr.index[1:N_TOP_FEATURES+1])  # skip 'target' itself
    top_neg = list(target_corr.tail(N_TOP_FEATURES).index)
    return {
        "n_rows": n_rows,
        "n_cols": n_cols,
        "missing": missing,
        "desc": desc,
        "corr": corr,
        "target_corr": target_corr,
        "top_pos": top_pos,
        "top_neg": top_neg
    }

def fit_models(X_train, X_test, y_train, y_test):
    """Fit several models and return results and fitted objects."""
    results = {}

    # Standard pipeline for linear models
    def make_pipe(model):
        return Pipeline([
            ("scaler", StandardScaler()),
            ("model", model)
        ])

    models = {
        "LinearRegression": make_pipe(LinearRegression()),
        "Ridge": make_pipe(Ridge(random_state=RANDOM_STATE)),
        "Lasso": make_pipe(Lasso(random_state=RANDOM_STATE, max_iter=10000)),
        "RandomForest": RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE)
    }

    for name, pipe in models.items():
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        # 5-fold CV R2 (on full X,y for stable comparison)
        try:
            cv_r2 = cross_val_score(pipe, np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)), cv=5, scoring='r2')
            cv_r2_mean = cv_r2.mean()
            cv_r2_std = cv_r2.std()
        except Exception:
            cv_r2_mean = np.nan
            cv_r2_std = np.nan

        results[name] = {
            "pipe": pipe,
            "y_pred": y_pred,
            "r2_test": r2,
            "mse_test": mse,
            "cv_r2_mean": cv_r2_mean,
            "cv_r2_std": cv_r2_std
        }

    # Compute feature importances for RandomForest if possible
    rf = results["RandomForest"]["pipe"]
    if hasattr(rf, "feature_importances_"):
        results["RandomForest"]["feature_importances"] = rf.feature_importances_
    return results

# ------------- Generate plots (return matplotlib figures) -------------
def plot_data_preview(df):
    fig, ax = plt.subplots(figsize=(11, 6))
    ax.axis("off")
    table_data = df.head(10).round(3)
    tbl = ax.table(cellText=table_data.values, colLabels=table_data.columns, loc='center')
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(8)
    tbl.scale(1, 1.2)
    ax.set_title("Data Preview (first 10 rows)")
    return fig

def plot_corr_heatmap(corr):
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr, ax=ax, cmap='coolwarm', center=0, square=True, cbar_kws={'shrink': .6})
    ax.set_title("Feature Correlation Matrix")
    return fig

def plot_top_feature_scatter(df, top_features):
    fs = top_features[:4]  # up to 4 plots
    fig, axs = plt.subplots(2, 2, figsize=(11, 8))
    axs = axs.ravel()
    for i, feat in enumerate(fs):
        sns.scatterplot(x=df[feat], y=df['target'], ax=axs[i], s=20)
        axs[i].set_title(f"{feat} vs target (corr={df.corr()['target'][feat]:.3f})")
        axs[i].set_xlabel(feat)
        axs[i].set_ylabel("Disease progression")
    # Hide any unused axis
    for j in range(len(fs), 4):
        axs[j].axis('off')
    plt.tight_layout()
    return fig

def plot_target_distribution(df):
    fig, ax = plt.subplots(figsize=(10, 4))
    sns.histplot(df['target'], bins=30, kde=True, ax=ax)
    ax.set_title("Distribution of disease progression (target)")
    ax.set_xlabel("Target value (disease progression)")
    return fig

def plot_model_diagnostics(y_test, y_pred, model_name):
    fig, axs = plt.subplots(1, 2, figsize=(11, 4.5))
    axs[0].scatter(y_test, y_pred, s=20)
    axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    axs[0].set_title(f"Predicted vs Actual (Test set) â€” {model_name}")
    axs[0].set_xlabel("Actual target")
    axs[0].set_ylabel("Predicted target")

    residuals = y_test - y_pred
    sns.histplot(residuals, bins=25, kde=True, ax=axs[1])
    axs[1].set_title("Residuals distribution (Actual - Predicted)")
    axs[1].set_xlabel("Residual value")
    plt.tight_layout()
    return fig

def plot_feature_coefficients(pipe, X_columns, model_name):
    # If linear model with coef_ inside pipeline
    model = pipe.named_steps.get("model", None)
    coefs = None
    if model is not None and hasattr(model, "coef_"):
        coefs = model.coef_
    # If RandomForest, we expect feature_importances_ on the final estimator (handled separately)
    if coefs is not None:
        coeff_series = pd.Series(coefs, index=X_columns).sort_values(ascending=False)
        fig, ax = plt.subplots(figsize=(10, 4))
        coeff_series.plot(kind='bar', ax=ax)
        ax.set_title(f"{model_name} Coefficients")
        ax.set_ylabel("Coefficient value")
        plt.tight_layout()
        return fig
    else:
        # return empty fig if no coefficients available
        fig = plt.figure(figsize=(10, 3))
        fig.text(0.5, 0.5, f"No linear coefficients available for {model_name}", ha='center', va='center')
        return fig

def plot_rf_feature_importances(importances, X_columns):
    s = pd.Series(importances, index=X_columns).sort_values(ascending=False)
    fig, ax = plt.subplots(figsize=(10, 4))
    s.plot(kind='bar', ax=ax)
    ax.set_title("Random Forest Feature Importances")
    ax.set_ylabel("Importance")
    plt.tight_layout()
    return fig

# ------------- Optional Bayesian regression (simple linear with top features) -------------
def run_bayesian_regression(X_train, y_train, X_test, y_test, feature_names, out_dir):
    """Run a simple Bayesian linear regression using PyMC (if available). Save posterior plot to file."""
    if not HAS_PYMC:
        return None, "PyMC not installed; skipping Bayesian regression."

    with pm.Model() as model:
        # Standardize inputs manually for stability
        X_mean = X_train.mean(axis=0)
        X_std = X_train.std(axis=0)
        X_std_replaced = X_std.replace(0, 1)
        Xs = (X_train - X_mean) / X_std_replaced

        # Priors on coefficients
        coefs = pm.Normal("coefs", mu=0, sigma=1, shape=Xs.shape[1])
        intercept = pm.Normal("intercept", mu=0, sigma=1)
        sigma = pm.HalfNormal("sigma", sigma=1)

        mu = intercept + pm.math.dot(Xs, coefs)
        y_obs = pm.Normal("y_obs", mu=mu, sigma=sigma, observed=(y_train - y_train.mean()) / y_train.std())

        # Sample posterior (small draws to keep runtime reasonable)
        trace = pm.sample(1000, tune=1000, target_accept=0.9, return_inferencedata=True)

        # Posterior predictive for test set
        # Standardize test set with same mean/std:
        Xs_test = (X_test - X_mean) / X_std_replaced
        ppc = pm.sample_posterior_predictive(trace, model=model, var_names=["coefs", "intercept"])

    # Save summary plot
    try:
        az_plot = az.plot_forest(trace, var_names=["coefs", "intercept"], combined=True)
        fig = az_plot.figure
        bayes_fig_path = os.path.join(out_dir, "bayesian_coefs.png")
        fig.savefig(bayes_fig_path, bbox_inches='tight')
        plt.close(fig)
        return bayes_fig_path, None
    except Exception as e:
        return None, f"PyMC ran but plotting failed: {e}"

# ------------- Main script -------------
def main():
    # 1. Load
    df, raw = load_and_prepare()

    # 2. Summarize
    summary = summarize_data(df)
    n_rows = summary["n_rows"]
    n_cols = summary["n_cols"]
    date_generated = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 3. Prepare features and train/test split
    # Use SelectKBest to pick top features automatically (super simple)
    X_all = df.drop(columns=["target"])
    y_all = df["target"]

    selector = SelectKBest(score_func=f_regression, k=N_TOP_FEATURES)
    selector.fit(X_all, y_all)
    selected_mask = selector.get_support()
    top_features = X_all.columns[selected_mask].tolist()

    X = df[top_features]
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)

    # 4. Fit models
    results = fit_models(X_train, X_test, y_train, y_test)

    # 5. Create PDF report with figures
    with PdfPages(PDF_FILENAME) as pdf:
        # Title page
        fig = plt.figure(figsize=(11, 8.5))
        fig.patch.set_facecolor('white')
        plt.axis('off')
        plt.text(0.5, 0.78, "Diabetes Data Insight Report", ha='center', va='center', fontsize=26, weight='bold', color='#2E86AB')
        plt.text(0.5, 0.72, "Feature analysis, models, diagnostics and recommendations", ha='center', va='center', fontsize=14)
        plt.text(0.5, 0.60, f"Rows: {n_rows} | Columns: {n_cols} | Generated: {date_generated}", ha='center', va='center', fontsize=10)
        plt.text(0.5, 0.12, "Generated by Insight Hub Analysis Script", ha='center', va='center', fontsize=9, color='gray')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Data preview
        pdf.savefig(plot_data_preview(df), bbox_inches='tight')
        plt.close('all')

        # Correlation heatmap
        pdf.savefig(plot_corr_heatmap(summary["corr"]), bbox_inches='tight')
        plt.close('all')

        # Top feature scatter plots
        pdf.savefig(plot_top_feature_scatter(df, top_features), bbox_inches='tight')
        plt.close('all')

        # Target distribution
        pdf.savefig(plot_target_distribution(df), bbox_inches='tight')
        plt.close('all')

        # Model diagnostics and metrics summary
        # Add a page summarizing metrics
        fig = plt.figure(figsize=(11, 8.5))
        plt.axis('off')
        plt.text(0.05, 0.9, "Model Performance Summary", fontsize=18, weight='bold', color='#2E86AB')
        y_text = 0.82
        for name, res in results.items():
            line = f"{name}: Test R^2 = {res['r2_test']:.3f}, Test MSE = {res['mse_test']:.1f}, CV R^2 mean = {res['cv_r2_mean']:.3f}"
            plt.text(0.05, y_text, line, fontsize=11)
            y_text -= 0.05
        plt.text(0.05, y_text - 0.02, "Top selected features used for these models: " + ", ".join(top_features), fontsize=10)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # For each model add diagnostics
        for name, res in results.items():
            pdf.savefig(plot_model_diagnostics(y_test, res['y_pred'], name), bbox_inches='tight')
            plt.close('all')
            # If linear model, show coefficients
            if name in ("LinearRegression", "Ridge", "Lasso"):
                try:
                    pdf.savefig(plot_feature_coefficients(res['pipe'], X.columns, name), bbox_inches='tight')
                except Exception:
                    pass
                plt.close('all')

        # Random forest importances if available
        if "feature_importances" in results["RandomForest"]:
            pdf.savefig(plot_rf_feature_importances(results["RandomForest"]["feature_importances"], X.columns), bbox_inches='tight')
            plt.close('all')

        # Executive summary page
        fig = plt.figure(figsize=(11, 8.5))
        plt.axis('off')
        plt.text(0.05, 0.9, "Executive Summary", fontsize=18, weight='bold', color='#2E86AB')
        y = 0.82
        lines = [
            f"Dataset: scikit-learn diabetes dataset (rows={n_rows}, cols={n_cols})",
            f"Selected top {N_TOP_FEATURES} features (SelectKBest f_regression): {', '.join(top_features)}",
            "",
            "Key findings:",
            f"- BMI-related and serum measurements strongly correlate with disease progression.",
            f"- Top features used: {', '.join(top_features)}.",
            f"- Best model (by test R^2) : " + max(results.items(), key=lambda kv: kv[1]['r2_test'])[0],
            f"- Recommendation: try regularized models (Ridge/Lasso) and tree-based models for better performance.",
            "",
            "Notes:",
            "- This report shows baseline models and diagnostics. For production, do feature engineering, cross-validated hyperparameter tuning, and consider longitudinal data."
        ]
        for ln in lines:
            plt.text(0.05, y, ln, fontsize=11)
            y -= 0.04
            if y < 0.06:
                pdf.savefig(fig, bbox_inches='tight')
                plt.close(fig)
                fig = plt.figure(figsize=(11, 8.5))
                plt.axis('off')
                y = 0.95
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Appendix: summary statistics
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')
        desc = summary["desc"].round(3)
        tbl = ax.table(cellText=desc.values, colLabels=desc.columns, rowLabels=desc.index, loc='center')
        tbl.auto_set_font_size(False)
        tbl.set_fontsize(8)
        tbl.scale(1, 1.2)
        plt.title("Summary statistics (features + target)")
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Optional: Bayesian regression if PyMC is installed
        if HAS_PYMC:
            try:
                bayes_fig_path, err = run_bayesian_regression(X_train, y_train, X_test, y_test, X.columns, OUT_DIR)
                if bayes_fig_path:
                    # Insert the saved posterior figure into PDF
                    fig = plt.figure(figsize=(11, 8.5))
                    plt.axis('off')
                    img = plt.imread(bayes_fig_path)
                    plt.imshow(img)
                    plt.axis('off')
                    pdf.savefig(fig, bbox_inches='tight')
                    plt.close(fig)
                else:
                    # note about PyMC skip or errors
                    fig = plt.figure(figsize=(11, 8.5))
                    plt.axis('off')
                    plt.text(0.5, 0.5, f"Bayesian regression not completed: {err}", ha='center')
                    pdf.savefig(fig, bbox_inches='tight')
                    plt.close(fig)
            except Exception as e:
                fig = plt.figure(figsize=(11, 8.5))
                plt.axis('off')
                plt.text(0.5, 0.5, f"Bayesian regression failed: {e}", ha='center')
                pdf.savefig(fig, bbox_inches='tight')
                plt.close(fig)
        else:
            fig = plt.figure(figsize=(11, 8.5))
            plt.axis('off')
            plt.text(0.5, 0.6, "Bayesian regression (PyMC) not available", ha='center', fontsize=14)
            plt.text(0.5, 0.52, "To enable: install PyMC and arviz (pip install pymc arviz)", ha='center')
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

        # Footer / metadata page
        fig = plt.figure(figsize=(11, 8.5))
        plt.axis('off')
        plt.text(0.5, 0.6, "End of Report", ha='center', fontsize=16, weight='bold')
        plt.text(0.5, 0.52, f"Report generated on {date_generated}", ha='center')
        plt.text(0.5, 0.46, "Generated by Insight Hub Analysis Script by Mwenda E. Njagi", ha='center', color='gray')
        plt.text(0.5, 0.40, "Data source: scikit-learn diabetes dataset", ha='center', color='gray')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

    print("âœ… PDF report generated at:", PDF_FILENAME)
    # Print summary to console
    print("Top features used:", top_features)
    for name, res in results.items():
        print(f"{name} â€” Test R^2: {res['r2_test']:.3f}, Test MSE: {res['mse_test']:.2f}, CV R^2 mean: {res['cv_r2_mean']:.3f}")

if __name__ == "__main__":
    main()